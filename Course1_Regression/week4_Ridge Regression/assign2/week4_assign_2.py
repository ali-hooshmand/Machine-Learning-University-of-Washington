# coding=utf-8# TASKS:================================================================================================================# Regression Week 4: Ridge Regression Assignment 2# In this assignment, you will implement ridge regression via gradient descent. You will:# - Convert an dataframe into a Numpy array# - Write a Numpy function to compute the derivative of the regression weights with respect to a single feature# - Write gradient descent function to compute the regression weights given an initial weight vector,#   step size, tolerance, and L2 penalty# ======================================================================================================================import pandasimport numpy as npimport timefrom sklearn import linear_modelstart = time.time()dtype_dict = {'bathrooms':float, 'waterfront':int, 'sqft_above':int, 'sqft_living15':float, 'grade':int, 'yr_renovated':int, 'price':float, 'bedrooms':float, 'zipcode':str, 'long':float, 'sqft_lot15':float, 'sqft_living':float, 'floors':str, 'condition':int, 'lat':float, 'date':str, 'sqft_basement':int, 'yr_built':int, 'id':str, 'sqft_lot':int, 'view':int}# ======================================================================================================================# 1.  Load in the house data:sales = pandas.read_csv('./kc_house_data.csv')# ======================================================================================================================# 2 & 3. From Module 2, copy and paste the ‘get_numpy_data’ function (or equivalent) that takes a dataframe, a list of# features (e.g. [‘sqft_living’, ‘bedrooms’]), to be used as inputs, and a name of the output (e.g. ‘price’).# This function returns a ‘feature_matrix’ (2D array) consisting of first a column of ones followed by columns# containing the values of the input features in the data set in the same order as the input list. It also returns an# ‘output_array’ which is an array of the values of the output in the data set (e.g. ‘price’).def get_numpy_data(data, features, output):    data['constant'] = 1    features = ['constant'] + features # features.insert(0, 'constant')    feature_matrix = np.asanyarray(data[features])    output_array = np.asanyarray(data[output])    return feature_matrix, output_array# ======================================================================================================================# 4. Copy and paste the ‘predict_output’ function (or equivalent) from Module 2. This function accepts a# 2D array ‘feature_matrix’ and a 1D array ‘weights’ and return a 1D array ‘predictions’.def predict_output(feature_matrix, weights):    predictions = np.dot(feature_matrix, weights)    return predictions# ======================================================================================================================# 5. We are now going to move to computing the derivative of the regression cost function. Recall that the cost function#  is the sum over the data points of the squared difference between an observed output and a predicted output,#  plus the L2 penalty term.# Cost(w) = SUM[ (prediction - output)^2 ]+ l2_penalty*(w[0]^2 + w[1]^2 + ... + w[k]^2).# The derivative with respect to w[i] is:# 2 * SUM[ error * [feature_i] ] + 2 * l2_penalty * w[i]# IMPORTANT: We will not regularize the constant. Thus, in the case of the constant,# the derivative is just twice the sum of the errors (without the 2*l2_penalty*w[0] term).# ======================================================================================================================# 6. With this in mind write the derivative function which computes the derivative of the weight given the value of the# feature (over all data points) and the errors (over all data points). To decide when to we are dealing with the# constant (so we don't regularize it) we added the extra parameter to the call ‘feature_is_constant’ which you should# set to True when computing the derivative of the constant and False otherwise.def feature_derivative_ridge(errors, feature, weight, l2_penalty, feature_is_constant):    if feature_is_constant:        derivative = 2 * np.dot(errors, feature)    else:        derivative = 2 * np.dot(errors, feature) + 2 * l2_penalty * weight    return derivative# ======================================================================================================================# 7. To test your feature derivative function, run the following:(example_features, example_output) = get_numpy_data(sales, ['sqft_living'], 'price')my_weights = np.array([1., 10.])test_predictions = predict_output(example_features, my_weights)errors = test_predictions - example_output # prediction errors# next two lines should print the same valuesprint feature_derivative_ridge(errors, example_features[:,1], my_weights[1], 1, False)print np.sum(errors*example_features[:, 1])*2+20.print ''# -5.6554166816e+13# -5.6554166816e+13# next two lines should print the same valuesprint feature_derivative_ridge(errors, example_features[:,0], my_weights[0], 1, True)print np.sum(errors)*2.# -22446749330.0# -22446749330.0# ======================================================================================================================# 8. Now we will write a function that performs a gradient descent. The basic premise is simple. Given a starting point# we update the current weights by moving in the negative gradient direction. Recall that the gradient is the direction# of increase and therefore the negative gradient is direction of decrease and we're trying to minimize a cost function.# The amount by which we move in the negative gradient direction is called the ‘step size’. We stop when we are# ‘sufficiently close’ to the optimum. Unlike in Module 2, this time we will set a maximum number of iterations and take#  gradient steps until we reach this maximum number. If no maximum number is supplied, the maximum should be set 100# by default. (Use default parameter values in Python.)# With this in mind, write a gradient descent function using your derivative function above.# For each step in the gradient descent, we update the weight for each feature before computing our stopping criteria.# The function will take the following parameters:# - 2D feature matrix# - array of output values# - initial weights# - step size# - L2 penalty# - maximum number of iterationsdef ridge_regression_gradient_descent(feature_matrix, output, initial_weights, step_size, l2_penalty, max_iterations=100):    weights = np.array(initial_weights) # make sure it's a numpy array    # while not reached maximum number of iterations:    iteration = 0    while iteration < max_iterations:        # compute the predictions using your predict_output() function        predictions = predict_output(feature_matrix, weights)        # compute the errors as predictions - output        errors = predictions - output        for i in xrange(len(weights)): # loop over each weight            # Recall that feature_matrix[:,i] is the feature column associated with weights[i]            # compute the derivative for weight[i].            # (Remember: when i=0, you are computing the derivative of the constant!)            if i == 0:                feature_is_constant = True            else:                feature_is_constant = False            derivative = feature_derivative_ridge(errors, feature_matrix[:, i], weights[i], l2_penalty, feature_is_constant)            # subtract the step size times the derivative from the current weight            weights[i] = weights[i] - step_size * derivative        iteration += 1    return weights# ======================================================================================================================# 9. The L2 penalty gets its name because it causes weights to have small L2 norms than otherwise. Let's see how large# weights get penalized. Let us consider a simple model with 1 feature.# - features: ‘sqft_living’# - output: ‘price’# ======================================================================================================================# 10. Load training set and test set.sales_train = pandas.read_csv('./wk3_kc_house_train_data.csv')sales_test = pandas.read_csv('./wk3_kc_house_test_data.csv')# ======================================================================================================================# 11. Convert the training set and test set using the ‘get_numpy_data’ function.e.g. in Python:(sales_train_matrix, output_train_array) = get_numpy_data(sales_train, ['sqft_living'], 'price')(sales_test_matrix,  output_test_array)  = get_numpy_data(sales_test,  ['sqft_living'], 'price')# ======================================================================================================================# 12. First, let’s consider no regularization. Set the L2 penalty to 0.0 and run your ridge regression algorithm to# learn the weights of the simple model (described above). Use the following parameters:# - step_size = 1e-12# - max_iterations = 1000# - initial_weights = all zeros# - Store the learned weights as simple_weights_0_penaltyinitial_weights = np.asarray([0.0, 0.0])step_size = 1e-12l2_penalty = 0.0max_iterations = 1000simple_weights_0_penalty = ridge_regression_gradient_descent(sales_train_matrix,                                                             output_train_array,                                                             initial_weights,                                                             step_size, l2_penalty,                                                             max_iterations=100)# ======================================================================================================================# 13. Next, let’s consider high regularization. Set the L2 penalty to 1e11 and run your ridge regression to learn the# weights of the simple model. Use the same parameters as above.l2_penalty =1e11simple_weights_high_penalty = ridge_regression_gradient_descent(sales_train_matrix,                                                                output_train_array,                                                                initial_weights,                                                                step_size, l2_penalty,                                                                max_iterations=100)# ======================================================================================================================# 14. If you have access to matplotlib, the following piece of code will plot the two learned models.# (The blue line is for the model with no regularization and the red line is for the one with high regularization.)import matplotlib.pyplot as pltplt.plot(sales_train_matrix,output_train_array,'k.',         sales_train_matrix,predict_output(sales_train_matrix, simple_weights_0_penalty),'b-',         sales_train_matrix,predict_output(sales_train_matrix, simple_weights_high_penalty),'r-')plt.show(block=True)# ======================================================================================================================# 15. Quiz Question:# What is the value of the coefficient for sqft_living that you learned with no regularization,# rounded to 1 decimal place? What about the one with high regularization?print simple_weights_0_penalty[0]print simple_weights_0_penalty[1]# Answer : [0, 264.7]print simple_weights_high_penalty[0]print simple_weights_high_penalty[1]# Answer : [0, 89.3]# ======================================================================================================================# 16. Quiz Question:# Comparing lines you fit with the with no regularization versus high regularization, which one is steeper?# Answer: The line with no regularization# ======================================================================================================================# 17. Compute the RSS on the TEST data for the following three sets of weights:# - The initial weights (all zeros)predictions1 = predict_output(sales_test_matrix, np.asarray([0, 0]))errors1 = predictions1 - output_test_arrayRSS_1 = np.dot(errors1, errors1)# - The weights learned with no regularizationpredictions2 = predict_output(sales_test_matrix, simple_weights_0_penalty)errors2 = predictions2 - output_test_arrayRSS_2 = np.dot(errors2, errors2)# - The weights learned with high regularizationpredictions3 = predict_output(sales_test_matrix, simple_weights_high_penalty)errors3 = predictions3 - output_test_arrayRSS_3 = np.dot(errors3, errors3)# ======================================================================================================================# 18. Quiz Question:# What are the RSS on test data for each of the set of weights above (initial, no regularization, high regularization)?print 'RSS for zero weights: ' + str(RSS_1)print 'RSS for no regularized weights: ' + str(RSS_2)print 'RSS for high regularized weights: ' + str(RSS_3)# Answer:# RSS for zero weights: 9.28e+14# RSS for no regularized weights: 1.43e+14# RSS for high regularized weights: 4.862e+14# ======================================================================================================================# 19. Let us now consider a model with 2 features: [ ‘sqft_living’, ‘sqft_living_15’].# First, create Numpy version of your training and test data with the two features.model_features = ['sqft_living', 'sqft_living15']my_output = 'price'(feature_matrix, output) = get_numpy_data(sales_train, model_features, my_output)(test_feature_matrix, test_output) = get_numpy_data(sales_test, model_features, my_output)# ======================================================================================================================# 20. First, let’s consider no regularization. Set the L2 penalty to 0.0 and run your ridge regression algorithm.# Use the following parameters:initial_weights = np.asarray([0.0, 0.0, 0.0])step_size = 1e-12max_iterations = 1000l2_penalty = 0.0multiple_weights_0_penalty = ridge_regression_gradient_descent(feature_matrix,                                                               output,                                                               initial_weights,                                                               step_size, l2_penalty,                                                               max_iterations=100)# ======================================================================================================================# 21. Next, let’s consider high regularization. Set the L2 penalty to 1e11 and run your ridge regression to learn the# weights of the simple model. Use the same parameters as above.l2_penalty = 1e11multiple_weights_high_penalty = ridge_regression_gradient_descent(feature_matrix,                                                                  output,                                                                  initial_weights,                                                                  step_size, l2_penalty,                                                                  max_iterations=100)# ======================================================================================================================# 22. Quiz Question:# What is the value of the coefficient for ‘sqft_living’ that you learned with no regularization?# What about the one with high regularization?print 'multiple_weights_0_penalty sqft_living weight is: ' + str(multiple_weights_0_penalty[1])print 'multiple_weights_high_penalty sqft_living weight is: ' + str(multiple_weights_high_penalty[1])# Answer:# multiple_weights_0_penalty sqft_living weight is: 174.0# multiple_weights_high_penalty sqft_living weight is: 70.8# ======================================================================================================================# 23. Compute the RSS on the TEST data for the following three sets of weights:# The initial weights (all zeros)predictions1 = predict_output(test_feature_matrix, np.asarray([0, 0, 0]))errors1 = predictions1 - test_outputRSS_1 = np.dot(errors1, errors1)# - The weights learned with no regularizationpredictions2 = predict_output(test_feature_matrix, multiple_weights_0_penalty)errors2 = predictions2 - test_outputRSS_2 = np.dot(errors2, errors2)# - The weights learned with high regularizationpredictions3 = predict_output(test_feature_matrix, multiple_weights_high_penalty)errors3 = predictions3 - test_outputRSS_3 = np.dot(errors3, errors3)# ======================================================================================================================# 24. Quiz Question:# What are the RSS on the test data for each of the set of weights above# (initial, no regularization, high regularization)?print 'RSS for zero weights: ' + str(RSS_1)print 'RSS for no regularized weights: ' + str(RSS_2)print 'RSS for high regularized weights: ' + str(RSS_3)# RSS for zero weights: 9.28e+14# RSS for no regularized weights: 1.47e+14# RSS for high regularized weights: 3.57e+14# ======================================================================================================================# 25. Predict the house price for the 1st house in the test set using the no regularization and high regularization# models. (Remember that python starts indexing from 0.)prediction_1st_house_nol2 = predict_output(test_feature_matrix[0, :], multiple_weights_0_penalty)prediction_1st_house_highl2 = predict_output(test_feature_matrix[0, :], multiple_weights_high_penalty)# ======================================================================================================================# 26. Quiz Question:# What's the error in predicting the price of the first house in the test set using the weights# learned with no regularization? What about with high regularization?print test_output[0] # $323000print errors2[0]  # error2 = $242932.0print errors3[0]  # error3 =  $41858.7print prediction_1st_house_nol2 - test_output[0]    # $242932.0print prediction_1st_house_highl2 - test_output[0]  #  $41858.7end = time.time()print('Time of Process was: ' + str(end - start) + '[sec]')