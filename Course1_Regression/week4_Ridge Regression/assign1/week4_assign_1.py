# coding=utf-8# TASKS:================================================================================================================# Regression Week 4: Ridge Regression Assignment 1# In this assignment, we will run ridge regression multiple times with different L2 penalties to see which one produces# the best fit. We will revisit the example of polynomial regression as a means to see the effect of L2 regularization.# In particular, we will:# - Use a pre-built implementation of regression to run polynomial regression# - Use matplotlib to visualize polynomial regressions# - Use a pre-built implementation of regression to run polynomial regression, this time with L2 penalty# - Use matplotlib to visualize polynomial regressions under L2 regularization# - Choose best L2 penalty using cross-validation.# - Assess the final fit using test data.# - We will continue to use the House data from previous assignments. (In the next programming assignment for this# module, you will implement your own ridge regression learning algorithm using gradient descent.)# ======================================================================================================================import pandasimport numpy as npimport timefrom sklearn import linear_modelstart = time.time()# ======================================================================================================================# 1. Copy and paste an equivalent of ‘polynomial_sframe’ function from Module 3 (Polynomial Regression). This function# accepts an array ‘feature’ (of type pandas.Series) and a maximal ‘degree’ and returns an data frame (of type pandas.# DataFrame) with the first column equal to ‘feature’ and the remaining columns equal to ‘feature’ to increasing integer# powers up to ‘degree’.def polynomial_dataframe(feature, degree):  # feature is pandas.Series type    # assume that degree >= 1    # initialize the dataframe:    poly_dataframe = pandas.DataFrame(dtype='double')    # and set poly_dataframe['power_1'] equal to the passed feature    poly_dataframe['power_1'] = feature    for i in range(2, degree+1):        col_name = 'power_'+str(i)        poly_dataframe[col_name] = feature**i    return poly_dataframe# ======================================================================================================================# 2. For the remainder of the assignment we will be working with the house Sales data as in Module 3 (Polynomial# Regression). Load in the data and also sort the sales data frame by ‘sqft_living’. When we plot the fitted values we# want to join them up in a line and this works best if the variable on the X-axis (which will be ‘sqft_living’) is# sorted. For houses with identical square footage, we break the tie by their prices.sales = pandas.read_csv('./kc_house_data.csv')sales.sort_values(['sqft_living', 'price'])# ======================================================================================================================# 3. Let us revisit the 15th-order polynomial model using the 'sqft_living' input. Generate polynomial features up to# degree 15 using `polynomial_sframe()` and fit a model with these features. When fitting the model,# use an L2 penalty of 1.5e-5:polydata_15 = polynomial_dataframe(np.asanyarray(sales['sqft_living'], dtype=object), 15)l2_small_penalty = 1.5e-5# 1- Create linear regression objectridge = linear_model.Ridge(alpha=l2_small_penalty, normalize=True)# 2- Train the model using l2 penaltyridge.fit(np.asanyarray(polydata_15), np.asanyarray(sales['price']))# ======================================================================================================================# 4. Quiz Question: What’s the learned value for the coefficient of feature power_1?print 'the coefficient of feature power_1 is ' + str(ridge.coef_[0])# Answer: 124.8733# ======================================================================================================================# Recall from Module 3 (Polynomial Regression) that the polynomial fit of degree 15 changed wildly whenever the data# changed. In particular, when we split the sales data into four subsets and fit the model of degree 15, the result came#  out to be very different for each subset. The model had a high variance. We will see in a moment that ridge# regression reduces such variance. But first, we must reproduce the experiment we did in Module 3.# ======================================================================================================================# 6. Download the provided csv files for each subset and load them with the given list of types:sales_set1 = pandas.read_csv('wk3_kc_house_set_1_data.csv')sales_set2 = pandas.read_csv('wk3_kc_house_set_2_data.csv')sales_set3 = pandas.read_csv('wk3_kc_house_set_3_data.csv')sales_set4 = pandas.read_csv('wk3_kc_house_set_4_data.csv')# ======================================================================================================================# 7. Just as we did in Module 3 (Polynomial Regression), fit a 15th degree polynomial on each of the 4 sets, plot the# results and view the weights for the four models. This time, setl2_small_penalty = 1e-9ridge = linear_model.Ridge(alpha=l2_small_penalty, normalize=True)sales_set1 = sales_set1.sort_values(['sqft_living', 'price'])poly15_data = polynomial_dataframe(np.asanyarray(sales_set1['sqft_living'], dtype=object), 15)poly15_data['price'] = np.asanyarray(sales_set1['price'], dtype=object)features_list = list(poly15_data)del features_list[-1]ridge.fit(poly15_data.as_matrix(features_list).reshape(len(poly15_data['power_1']), 15), np.asanyarray(poly15_data['price']))print 'Based on set1 data, the coefficient of feature power_1 is ' + str(ridge.coef_[0])sales_set2 = sales_set2.sort_values(['sqft_living', 'price'])poly15_data = polynomial_dataframe(np.asanyarray(sales_set2['sqft_living'], dtype=object), 15)poly15_data['price'] = np.asanyarray(sales_set2['price'], dtype=object)features_list = list(poly15_data)del features_list[-1]ridge.fit(poly15_data.as_matrix(features_list).reshape(len(poly15_data['power_1']), 15), np.asanyarray(poly15_data['price']))print 'Based on set2 data, the coefficient of feature power_1 is ' + str(ridge.coef_[0])sales_set3 = sales_set3.sort_values(['sqft_living', 'price'])poly15_data = polynomial_dataframe(np.asanyarray(sales_set3['sqft_living'], dtype=object), 15)poly15_data['price'] = np.asanyarray(sales_set3['price'], dtype=object)features_list = list(poly15_data)del features_list[-1]ridge.fit(poly15_data.as_matrix(features_list).reshape(len(poly15_data['power_1']), 15), np.asanyarray(poly15_data['price']))print 'Based on set3 data, the coefficient of feature power_1 is ' + str(ridge.coef_[0])sales_set4 = sales_set4.sort_values(['sqft_living', 'price'])poly15_data = polynomial_dataframe(np.asanyarray(sales_set4['sqft_living'], dtype=object), 15)poly15_data['price'] = np.asanyarray(sales_set4['price'], dtype=object)features_list = list(poly15_data)del features_list[-1]ridge.fit(poly15_data.as_matrix(features_list).reshape(len(poly15_data['power_1']), 15), np.asanyarray(poly15_data['price']))print 'Based on set4 data, the coefficient of feature power_1 is ' + str(ridge.coef_[0])# ======================================================================================================================# 8. Quiz Question: For the models learned in each of these training sets, what are the smallest and largest values you# learned for the coefficient of feature power_1? (For the purpose of answering this question, negative numbers are# considered "smaller" than positive numbers. So -5 is smaller than -3, and -3 is smaller than 5 and so forth.)# Answer:# Based on set1 data, the coefficient of feature power_1 is 544.669412961# Based on set2 data, the coefficient of feature power_1 is 859.362510246# Based on set3 data, the coefficient of feature power_1 is -755.395832641# Based on set4 data, the coefficient of feature power_1 is 1119.44568645# ======================================================================================================================# Ridge regression comes to rescue# ======================================================================================================================# ======================================================================================================================# 9. Generally, whenever we see weights change so much in response to change in data, we believe the variance of our# estimate to be large. Ridge regression aims to address this issue by penalizing "large" weights. (The weights looked# quite small, but they are not that small because 'sqft_living' input is in the order of thousands.)# ======================================================================================================================# 10. Fit a 15th-order polynomial model on set_1, set_2, set_3, and set_4, this time with a large L2 penalty.# Make sure to add "alpha=l2_large_penalty" and "normalize=True" to the parameter list,# where the value of l2_large_penalty is given byl2_large_penalty=1.23e2ridge = linear_model.Ridge(alpha=l2_large_penalty, normalize=True)poly15_data = polynomial_dataframe(np.asanyarray(sales_set1['sqft_living'], dtype=object), 15)poly15_data['price'] = np.asanyarray(sales_set1['price'], dtype=object)features_list = list(poly15_data)del features_list[-1]ridge.fit(poly15_data.as_matrix(features_list).reshape(len(poly15_data['power_1']), 15), np.asanyarray(poly15_data['price']))print 'Based on set1 data, the coefficient of feature power_1 is ' + str(ridge.coef_[0])poly15_data = polynomial_dataframe(np.asanyarray(sales_set2['sqft_living'], dtype=object), 15)poly15_data['price'] = np.asanyarray(sales_set2['price'], dtype=object)features_list = list(poly15_data)del features_list[-1]ridge.fit(poly15_data.as_matrix(features_list).reshape(len(poly15_data['power_1']), 15), np.asanyarray(poly15_data['price']))print 'Based on set2 data, the coefficient of feature power_1 is ' + str(ridge.coef_[0])poly15_data = polynomial_dataframe(np.asanyarray(sales_set3['sqft_living'], dtype=object), 15)poly15_data['price'] = np.asanyarray(sales_set3['price'], dtype=object)features_list = list(poly15_data)del features_list[-1]ridge.fit(poly15_data.as_matrix(features_list).reshape(len(poly15_data['power_1']), 15), np.asanyarray(poly15_data['price']))print 'Based on set3 data, the coefficient of feature power_1 is ' + str(ridge.coef_[0])poly15_data = polynomial_dataframe(np.asanyarray(sales_set4['sqft_living'], dtype=object), 15)poly15_data['price'] = np.asanyarray(sales_set4['price'], dtype=object)features_list = list(poly15_data)del features_list[-1]ridge.fit(poly15_data.as_matrix(features_list).reshape(len(poly15_data['power_1']), 15), np.asanyarray(poly15_data['price']))print 'Based on set4 data, the coefficient of feature power_1 is ' + str(ridge.coef_[0])# ======================================================================================================================# 11. QUIZ QUESTION: For the models learned with regularization in each of these training sets, what are the smallest# and largest values you learned for the coefficient of feature power_1? (For the purpose of answering this question,# negative numbers are considered "smaller" than positive numbers)# Answer:# Based on set1 data, the coefficient of feature power_1 is 2.32806802958# Based on set2 data, the coefficient of feature power_1 is 2.09756902778# Based on set3 data, the coefficient of feature power_1 is 2.28906258119# Based on set4 data, the coefficient of feature power_1 is 2.08596194092# ======================================================================================================================# Selecting an L2 penalty via cross-validation:# ======================================================================================================================# 12. Just like the polynomial degree, the L2 penalty is a "magic" parameter we need to select. We could use the# validation set approach as we did in the last module, but that approach has a major disadvantage: it leaves fewer# observations available for training. Cross-validation seeks to overcome this issue by using all of the training set# in a smart way.# We will implement a kind of cross-validation called k-fold cross-validation. The method gets its name because it# involves dividing the training set into k segments of roughtly equal size. Similar to the validation set method,# we measure the validation error with one of the segments designated as the validation set. The major difference is# that we repeat the process k times as follows:# - Set aside segment 0 as the validation set, and fit a model on rest of data, and evalutate it on this validation set# - Set aside segment 1 as the validation set, and fit a model on rest of data, and evalutate it on this validation set# ...# Set aside segment k-1 as the validation set, and fit a model on rest of data, and evalutate it on this validation set# After this process, we compute the average of the k validation errors, and use it as an estimate of the generalization# error. Notice that all observations are used for both training and validation, as we iterate over segments of data.# ======================================================================================================================# 13. To estimate the generalization error well, it is crucial to shuffle the training data before dividing them into# segments. We reserve 10% of the data as the test set and randomly shuffle the remainder.# Le'ts call the shuffled data 'train_valid_shuffled'.# For the purpose of this assignment, let us download the csv file containing pre-shuffled rows of training and# validation sets combined:sales_train_valid_shuffled = pandas.read_csv('./wk3_kc_house_train_valid_shuffled.csv')sales_test = pandas.read_csv('./wk3_kc_house_test_data.csv')# ======================================================================================================================# 14. Divide the combined training and validation set into equal segments.# Each segment should receive n/k elements, where n is the number of observations in the training set# and k is number of segments. Since segment 0 starts at index 0 and contains n/k elements, it ends at index (n/k)-1.# segment 1 starts where segment 0 left off, at index (n/k). With n/k elements, the segment 1 ends at index (n*2/k)-1.# Continuing in this fashion, we deduce that the segment i starts at index (n*i/k) and ends at (n*(i+1)/k)-1.# With this pattern in mind, we write a short loop that prints the starting and ending indices of each segment,# just to make sure you are getting the splits right.n = len(sales_train_valid_shuffled)k = 10  # 10-fold cross-validationfor i in xrange(k):    start = (n*i)/k    end = (n*(i+1))/k-1    # print i, (start, end)# Let us familiarize ourselves with array slicing with Pandas. To extract a continuous slice from a DataFrame,# use colon in square brackets. For instance, the following cell extracts rows 0 to 9 of train_valid_shuffled:# train_valid_shuffled[0:10] # select rows 0 to 9# Notice that the first index (0) is included in the slice but the last index (10) is omitted.# If the observations are grouped into 10 segments, the segment i is given by# start = (n*i)/10# end = (n*(i+1))/10# train_valid_shuffled[start:end+1]# Meanwhile, to choose the remainder of the data that's not part of the segment i,# we select two slices (0:start) and (end+1:n) and paste them together:# train_valid_shuffled[0:start].append(train_valid_shuffled[end+1:n])# ======================================================================================================================# Now we are ready to implement k-fold cross-validation. Write a function that computes k validation errors by# designating each of the k segments as the validation set. It accepts as parameters# (i) k,# (ii) l2_penalty,# (iii) dataframe containing input features (e.g. poly15_data) and# (iv) column of output values (e.g. price).# The function returns the average validation error using k segments as validation sets.# We shall assume that the input dataframe does not contain the output column.# For each i in [0, 1, ... k-1]:# - Compute starting and ending indices of segment i and call 'start' and 'end'# - Form validation set by taking a slice (start:end+1) from the data.# - Form training set by appending slice (end+1:n) to the end of slice (0:start).# - Train a linear model using training set just formed, with a given l2_penalty# - Compute validation error (RSS) using validation set just formeddef k_fold_cross_validation(k, l2_penalty, data, output):    average_validation_error = 0    n = data.shape[0]    ridge = linear_model.Ridge(alpha=l2_penalty, normalize=True)    for i in range(0, k):        start = (n * i) / k        end = (n * (i + 1)) / k - 1        validation_set = data[start:end+1]        validation_output = output[start:end+1]        # training_set = data[0:start].append(data[end+1:n])        training_set = np.vstack((data[0:start], data[end+1:n]))        # training_output = output[0:start].append(output[end+1:n])        training_output = np.append(output[0:start], output[end + 1:n])        ridge.fit(training_set, training_output)        predicted_output = ridge.predict(validation_set)        error = validation_output - predicted_output        average_validation_error += np.dot(error, error)/k    return average_validation_error# ======================================================================================================================# 16. Once we have a function to compute the average validation error for a model, we can write a loop to find the model#  that minimizes the average validation error. Write a loop that does the following:# - We will again be aiming to fit a 15th-order polynomial model using the sqft_living input# - For each l2_penalty in [10^3, 10^3.5, 10^4, 10^4.5, ..., 10^9] (to get this in Python, you can use this Numpy#   function: np.logspace(3, 9, num=13).): Run 10-fold cross-validation with l2_penalty.# - Report which L2 penalty produced the lowest average validation error.# Note: since the degree of the polynomial is now fixed to 15, to make things faster, you should generate polynomial# features in advance and re-use them throughout the loop. Make sure to use train_valid_shuffled when generating# polynomial features!l2_penalty_range = np.logspace(3, 9, num= 13, base= 10)val_err_array = [None] * 13index = 0k= 10data = polynomial_dataframe(np.asanyarray(sales_train_valid_shuffled['sqft_living'], dtype=object), 15)output = np.asanyarray(sales_train_valid_shuffled['price'], dtype=object)for l2_penalty in l2_penalty_range:    val_err_array[index] = k_fold_cross_validation(k, l2_penalty, data, output)    index += 1print val_err_arrayprint 'min of validation errors is : ' + str(min(val_err_array))print 'The L2 penalty with minimum validation error is: ' + str( l2_penalty_range[val_err_array.index(min(val_err_array)) ] )# ======================================================================================================================# 17. Quiz Question: What is the best value for the L2 penalty according to 10-fold validation?# Answer:# min of validation errors is : 2.64977361037e+14# The L2 penalty with minimum validation error is: 1000.0optimal_l2_penalty = l2_penalty_range[val_err_array.index(min(val_err_array)) ]# ======================================================================================================================# 18. Once you found the best value for the L2 penalty using cross-validation, it is important to retrain a final model# on all of training data using this value. This way, your final model will be trained on the entire dataset.sales_train = pandas.read_csv('./wk3_kc_house_train_data.csv')data = polynomial_dataframe(np.asanyarray(sales_train['sqft_living'], dtype=object), 15)output = np.asanyarray(sales_train['price'], dtype=object)ridge = linear_model.Ridge(alpha=optimal_l2_penalty, normalize=True)ridge.fit(data, output)test_input_data = polynomial_dataframe(np.asanyarray(sales_test['sqft_living'], dtype=object), 15)predicted_output_testdata = ridge.predict(test_input_data)error = np.asanyarray(sales_test['price']) - predicted_output_testdataRSS = np.dot(error, error)# ======================================================================================================================# 19. Quiz Question: Using the best L2 penalty found above, train a model using all training data.# What is the RSS on the TEST data of the model you learn with this L2 penalty?print 'RSS on test error is: ' + str(RSS)# RSS on test error is: 2.83756877068e+14end = time.time()print('Time of Process was: ' + str(end - start) + '[sec]')