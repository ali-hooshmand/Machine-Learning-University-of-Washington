# coding=utf-8# TASKS:================================================================================================================# Regression Week 5: LASSO Assignment 1# In this assignment, you will use LASSO to select features, building on a pre-implemented solver for LASSO.# You will:# - Run LASSO with different L1 penalties.# - Choose best L1 penalty using a validation set.# - Choose best L1 penalty using a validation set, with additional constraint on the size of subset.# In the second assignment, you will implement your own LASSO solver, using coordinate descent.# ======================================================================================================================import pandas as pdimport numpy as npimport timefrom sklearn import linear_modelfrom collections import OrderedDictstart = time.time()# ======================================================================================================================# 0. Load the sales dataset using Pandas:dtype_dict = {'bathrooms':float, 'waterfront':int, 'sqft_above':int, 'sqft_living15':float, 'grade':int,              'yr_renovated':int, 'price':float, 'bedrooms':float, 'zipcode':str, 'long':float, 'sqft_lot15':float,              'sqft_living':float, 'floors':float, 'condition':int, 'lat':float, 'date':str, 'sqft_basement':int,              'yr_built':int, 'id':str, 'sqft_lot':int, 'view':int}sales = pd.read_csv('./kc_house_data.csv', dtype=dtype_dict)# ======================================================================================================================# 1. Create new features by performing following transformation on inputs:# NOTES:# Squaring bedrooms will increase the separation between not many bedrooms (e.g. 1) and lots of bedrooms (e.g. 4)# since 1^2 = 1 but 4^2 = 16. Consequently this variable will mostly affect houses with many bedrooms.# On the other hand, taking square root of sqft_living will decrease the separation between big house and small house.# The owner may not be exactly twice as happy for getting a house that is twice as big.from math import log, sqrtsales['sqft_living_sqrt'] = sales['sqft_living'].apply(sqrt)sales['sqft_lot_sqrt'] = sales['sqft_lot'].apply(sqrt)sales['bedrooms_square'] = sales['bedrooms']*sales['bedrooms']sales['floors_square'] = sales['floors']*sales['floors']# ======================================================================================================================# 2. Using the entire house dataset, learn regression weights using an L1 penalty of 5e2. Make sure to add# "normalize=True" when creating the Lasso object. Refer to the following code snippet for the list of features.# Note: From here on, the list 'all_features' refers to the list defined in this snippet.all_features = ['bedrooms', 'bedrooms_square',                'bathrooms',                'sqft_living', 'sqft_living_sqrt',                'sqft_lot', 'sqft_lot_sqrt',                'floors', 'floors_square',                'waterfront', 'view', 'condition', 'grade',                'sqft_above',                'sqft_basement',                'yr_built', 'yr_renovated']model_all = linear_model.Lasso(alpha=5e2, normalize=True) # set parametersmodel_all.fit(sales[all_features], sales['price']) # learn weights# ======================================================================================================================# 3. Quiz Question: Which features have been chosen by LASSO, i.e. which features were assigned nonzero weights?print model_all.sparse_coef_# (0, 3)	134.439313955# (0, 10)	24750.0045856# (0, 12)	61749.1030908# Selected features:# - sqft_living# - view# - grade# ======================================================================================================================# 4. To find a good L1 penalty, we will explore multiple values using a validation set.# Let us do three way split into train, validation, and test sets.# Download the provided csv files containing training, validation and test sets.testing = pd.read_csv('wk3_kc_house_test_data.csv', dtype=dtype_dict)training = pd.read_csv('wk3_kc_house_train_data.csv', dtype=dtype_dict)validation = pd.read_csv('wk3_kc_house_valid_data.csv', dtype=dtype_dict)# Make sure to create the 4 features as we did in #1:testing['sqft_living_sqrt'] = testing['sqft_living'].apply(sqrt)testing['sqft_lot_sqrt'] = testing['sqft_lot'].apply(sqrt)testing['bedrooms_square'] = testing['bedrooms']*testing['bedrooms']testing['floors_square'] = testing['floors']*testing['floors']training['sqft_living_sqrt'] = training['sqft_living'].apply(sqrt)training['sqft_lot_sqrt'] = training['sqft_lot'].apply(sqrt)training['bedrooms_square'] = training['bedrooms']*training['bedrooms']training['floors_square'] = training['floors']*training['floors']validation['sqft_living_sqrt'] = validation['sqft_living'].apply(sqrt)validation['sqft_lot_sqrt'] = validation['sqft_lot'].apply(sqrt)validation['bedrooms_square'] = validation['bedrooms']*validation['bedrooms']validation['floors_square'] = validation['floors']*validation['floors']# ======================================================================================================================# 5. Now for each l1_penalty in [10^1, 10^1.5, 10^2, 10^2.5, ..., 10^7]l1_range = np.logspace(1, 7, num=13)# Learn a model on TRAINING data using the specified l1_penalty.# Make sure to specify normalize=True in the constructor:RSS = {'l1_penalty':[], 'rss':[]} # initiating an empty dictionary to log l1_penalties and rss valuesfor l1_penalty in l1_range:    model = linear_model.Lasso(alpha=l1_penalty, normalize=True)    model.fit(training[all_features], training['price'])    # Compute the RSS on VALIDATION for the current model    predictions = model.predict(validation[all_features])    errors = validation['price'] - predictions    rss = np.dot(errors, errors)    RSS['l1_penalty'].append(l1_penalty)    RSS['rss'].append(rss)print pd.DataFrame(RSS)#       l1_penalty           rss# 0   1.000000e+01  3.982133e+14# 1   3.162278e+01  3.990419e+14# 2   1.000000e+02  4.297916e+14# 3   3.162278e+02  4.637398e+14# 4   1.000000e+03  6.458987e+14# 5   3.162278e+03  1.222507e+15# 6   1.000000e+04  1.222507e+15# 7   3.162278e+04  1.222507e+15# 8   1.000000e+05  1.222507e+15# 9   3.162278e+05  1.222507e+15# 10  1.000000e+06  1.222507e+15# 11  3.162278e+06  1.222507e+15# 12  1.000000e+07  1.222507e+15# Report which L1 penalty produced the lower RSS on VALIDATION.# Answer: l1_penalty = 10# ======================================================================================================================# 6. Quiz Question: Which was the best value for the l1_penalty, i.e. which value of l1_penalty produced the lowest RSS# on VALIDATION data?# Answer: l1_penalty = 10# ======================================================================================================================# 7. Now that you have selected an L1 penalty, compute the RSS on TEST data for the model with the best L1 penalty.model = linear_model.Lasso(alpha=1e1, normalize=True)model.fit(training[all_features], training['price'])test_prediction = model.predict(testing[all_features])test_errors = test_prediction - testing['price']rss_test = np.dot(test_errors, test_errors)print rss_test# 9.84e+13# ======================================================================================================================# 8. Quiz Question: Using the best L1 penalty, how many nonzero weights do you have? Count the number of nonzero# coefficients first, and add 1 if the intercept is also nonzero. A succinct way to do this isprint 'total number of features are ' + str(len(model.coef_)+1)print 'number of nonzero elements is ' + str( np.count_nonzero(model.coef_) + np.count_nonzero(model.intercept_) )# total number of features are 18# number of nonzero elements is 15# ======================================================================================================================# 9. What if we absolutely wanted to limit ourselves to, say, 7 features? This may be important if we want to derive# "a rule of thumb" --- an interpretable model that has only a few features in them.# You are going to implement a simple, two phase procedure to achieve this goal:# - Explore a large range of ‘l1_penalty’ values to find a narrow region of ‘l1_penalty’ values where models are# likely to have the desired number of non-zero weights.# - Further explore the narrow region you found to find a good value for ‘l1_penalty’ that achieves the desired# sparsity. Here, we will again use a validation set to choose the best value for ‘l1_penalty’.# ======================================================================================================================# 10. Assign 7 to the variable ‘max_nonzeros’.max_nonzeros = 7# ======================================================================================================================# 11.Exploring large range of l1_penalty# For l1_penalty in np.logspace(1, 4, num=20):# - Fit a regression model with a given l1_penalty on TRAIN data. Add "alpha=l1_penalty" and "normalize=True".# - Extract the weights of the model and count the number of nonzeros. Take account of the intercept as we did in #8,# adding 1 whenever the intercept is nonzero. Save the number of nonzeros to a list.l1_range = np.logspace(1, 4, num=20)# Nonzeros = {'weight':[], 'nonzeros':[]}Nonzeros = OrderedDict([('l1_penalty', []), ('nonzeros', [])])for l1 in l1_range:    model = linear_model.Lasso(alpha=l1, normalize=True)    model.fit(training[all_features], training['price'])    Nonzeros['l1_penalty'].append(l1)    Nonzeros['nonzeros'].append(np.count_nonzero(model.coef_) + np.count_nonzero(model.intercept_))print pd.DataFrame(Nonzeros)'''      l1_penalty  nonzeros0      10.000000        151      14.384499        152      20.691381        153      29.763514        154      42.813324        135      61.584821        126      88.586679        117     127.427499        108     183.298071         79     263.665090         610    379.269019         611    545.559478         612    784.759970         513   1128.837892         314   1623.776739         315   2335.721469         216   3359.818286         117   4832.930239         118   6951.927962         119  10000.000000         1'''# ======================================================================================================================# 12. Out of this large range, we want to find the two ends of our desired narrow range of l1_penalty. At one end, we# have l1_penalty values that have too few non-zeros, & at other end, we have an l1_penalty that has too many non-zeros.# More formally, find:# - The largest l1_penalty that has more non-zeros than ‘max_nonzeros’ (if we pick a penalty smaller than this value,# we will definitely have too many non-zero weights)Store this value in the variable ‘l1_penalty_min’# - The smallest l1_penalty that has fewer non-zeros than ‘max_nonzeros’ (if we pick a penalty larger than this value,#  we will definitely have too few non-zero weights)Store this value in the variable ‘l1_penalty_max’# 13. Quiz Question:# What values did you find for l1_penalty_min and l1_penalty_max?l1_penalty_min = 127.427499l1_penalty_max = 263.665090# ======================================================================================================================# 14. Exploring narrower range of l1_penalty# We now explore the region of l1_penalty we found: between ‘l1_penalty_min’ and ‘l1_penalty_max’.# We look for the L1 penalty in this range that produces exactly the right number of nonzeros# and also minimizes RSS on the VALIDATION set.# For l1_penalty in np.linspace(l1_penalty_min,l1_penalty_max,20):# - Fit a regression model with a given l1_penalty on TRAIN data. Use "alpha=l1_penalty" and "normalize=True".# - Measure the RSS of the learned model on the VALIDATION set# - Find the model that the lowest RSS on the VALIDATION set and has sparsity equal to ‘max_nonzeros’. (Again, take# account of the intercept when counting the number of nonzeros.)RSS = OrderedDict([('l1_penalty', []), ('nonzeros', []),('rss', [])])for l1 in np.linspace(l1_penalty_min, l1_penalty_max, 20):    model = linear_model.Lasso(alpha=l1, normalize=True)    model.fit(training[all_features], training['price'])    predictions = model.predict(validation[all_features])    errors = predictions - validation['price']    rss = np.dot(errors, errors)    RSS['l1_penalty'].append(l1)    RSS['nonzeros'].append(np.count_nonzero(model.coef_)+np.count_nonzero(model.intercept_))    RSS['rss'].append(rss)print pd.DataFrame(RSS)'''    l1_penalty  nonzeros           rss0   127.427499        10  4.353747e+141   134.597899        10  4.370092e+142   141.768298         8  4.382361e+143   148.938698         8  4.391589e+144   156.109097         7  4.400374e+145   163.279497         7  4.407775e+146   170.449896         7  4.415667e+147   177.620296         7  4.424064e+148   184.790695         7  4.432967e+149   191.961095         7  4.442398e+1410  199.131494         7  4.452307e+1411  206.301894         6  4.462689e+1412  213.472293         6  4.471129e+1413  220.642693         6  4.479982e+1414  227.813092         6  4.489247e+1415  234.983492         6  4.498925e+1416  242.153891         6  4.509015e+1417  249.324291         6  4.519524e+1418  256.494690         6  4.530439e+1419  263.665090         6  4.541767e+14'''# ======================================================================================================================# 15. Quiz Question: What value of l1_penalty in our narrow range has the lowest RSS on the VALIDATION set and# has sparsity equal to ‘max_nonzeros’?# Answer: l1_penalty = 156.109097 with 7 nonzero coefficients and rss = 4.400374e+14# ======================================================================================================================# 16. Quiz Question: What features in this model have non-zero coefficients?model = linear_model.Lasso(alpha= 156.109097, normalize=True)model.fit(training[all_features], training['price'])for i in range(0, len(model.coef_)):    if model.coef_[i] != 0:        print all_features[i]# Answer: bathrooms, sqft_living, waterfront, view, grade, yr_builtend = time.time()print('Time of Process was: ' + str(end - start) + '[sec]')